{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "M:\\masters\\semester 4\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('queen', 0.8523603677749634)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import gensim\n",
    "\n",
    "#sentences = word2vec.Word2Vec.load()#try here to put the courpus of glove\n",
    "gensim_file='M:\\masters\\semester 4\\ProjectData\\Clean.glove.6B\\glove_model_6B_50d.txt'\n",
    "model=gensim.models.Word2Vec.load_word2vec_format(gensim_file,binary=False)\n",
    "model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'he' is to 'his' as 'she' is to 'her'\n",
      "'big' is to 'bigger' as 'bad' is to 'worse'\n",
      "'going' is to 'went' as 'being' is to 'was'\n"
     ]
    }
   ],
   "source": [
    "more_examples = [\"he his she\", \"big bigger bad\", \"going went being\"]\n",
    "for example in more_examples:\n",
    "    a, b, x = example.split()\n",
    "    predicted = model.most_similar([x, b], [a])[0][0]\n",
    "    print (\"'%s' is to '%s' as '%s' is to '%s'\" % (a, b, x, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import re\n",
    "\n",
    "BarkingDog = open(\"BarkingDog.txt\", \"r\") #input text file directory should go here\n",
    "text_file = open(\"TestVectorFile.txt\", \"w\") #output text file directory should go here\n",
    "text = BarkingDog.read()\n",
    "#print(text)\n",
    "split_text = text.split(\"\\n\") #splits the text into lines based on new lines\n",
    "split_lines = []\n",
    "for text in split_text:\n",
    "    split_lines = text.split(\" \") #array of the split text , that is re-split into an array of words\n",
    "    for word in split_lines:\n",
    "        clean_word = re.sub(r'[^a-zA-Z0-9 ]',r'', word )\n",
    "        print (clean_word)\n",
    "        if(clean_word != ''):\n",
    "            text_file.write(clean_word.lower()) #used s.lower() to convert the words to lower cases so it can be matched with glove\n",
    "            text_file.write(\" \")\n",
    "            for x in model[clean_word.lower()]:\n",
    "                value = str(x)\n",
    "                text_file.write(value)\n",
    "                text_file.write(\" \")\n",
    "        \n",
    "    split_lines.clear()\n",
    "text_file.close()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model[\"bidens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'isil'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-30080c7acffd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mtext_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_word\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#used s.lower() to convert the words to lower cases so it can be matched with glove\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mtext_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclean_word\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#this is where model[\"word\"] is called to get its vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                     \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0mtext_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mM:\\masters\\semester 4\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m   1301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mM:\\masters\\semester 4\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[1;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'isil'"
     ]
    }
   ],
   "source": [
    "import gensim, os, time\n",
    "\n",
    "directory = \"M:\\masters\\semester 4\\ProjectData\\SyriaSpeeches\" #read a certain text\n",
    "FileTextList = [] #where the text inside each file is saved, correpondes to the MP name in the MPName list index (actual list from syria speeches folder)\n",
    "FileNames = [] #where the file names would be saved inorder to be used later in the tag me files folder \n",
    "#annotations = [] #have all the entities annotated for a certain text file\n",
    "Word2vecFolderDirectory = \"M:\\masters\\semester 4\\ProjectData\\SyriaSpeechesWord2Vec\"\n",
    "\n",
    "\n",
    "gensim_file='M:\\masters\\semester 4\\ProjectData\\Clean.glove.6B\\glove_model_6B_50d.txt'\n",
    "model=gensim.models.Word2Vec.load_word2vec_format(gensim_file,binary=False)    #where the model is created\n",
    "    \n",
    "for file in os.listdir(directory): #save the text from the syria files\n",
    "    filedirectorylink = os.path.join(directory,file)\n",
    "    FileNames.append(file)\n",
    "    fileText = open(filedirectorylink, encoding=\"utf8\")\n",
    "    FileTextList.append(fileText.read())\n",
    "\n",
    "counter = 0;\n",
    "for file in FileTextList: #takes the text of each file\n",
    "    Word2vecFileDirectory = os.path.join(Word2vecFolderDirectory, FileNames[counter]) #creates the file directory\n",
    "    text_file = open(Word2vecFileDirectory, \"w\", encoding='utf-8')    #creates the file in that directory\n",
    "    split_text = file.split(\"|\") #splits the text into lines based separator \"|\"\n",
    "    split_lines = [] #an array to save the words from the split lines\n",
    "    for text in split_text:\n",
    "        split_lines = text.split(\" \") #array of the split text , that is re-split into an array of words\n",
    "        for word in split_lines: #loops each word in the current line\n",
    "            clean_word = re.sub(r'[^a-zA-Z0-9 ]',r'', word ) #cleans the word\n",
    "            #print (clean_word)\n",
    "            if(clean_word != ''): #check that if the word was actually a saved symbol to ignore it\n",
    "                text_file.write(clean_word.lower()) #used s.lower() to convert the words to lower cases so it can be matched with glove\n",
    "                text_file.write(\" \")\n",
    "                for x in model[clean_word.lower()]: #this is where model[\"word\"] is called to get its vector\n",
    "                    value = str(x)\n",
    "                    text_file.write(value)\n",
    "                    text_file.write(\" \")\n",
    "\n",
    "        split_lines.clear()\n",
    "    text_file.close()\n",
    "    counter = counter + 1 \n",
    "    print(\"Done file\" , Word2vecFileDirectory)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
